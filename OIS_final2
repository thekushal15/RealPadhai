import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.decomposition import PCA
import shap

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# Set seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class YieldLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=2, dropout=dropout, bidirectional=True)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, output_dim)
        )

    def forward(self, x):
        self.train()  # Always enable dropout (MC Dropout)
        out, _ = self.lstm(x)
        out = out[:, -1, :]
        return self.fc(out)

def create_sequences(df, input_cols, target_cols, window=7):
    X_seq, y_seq, D_seq = [], [], []
    for i in range(len(df) - window):
        X_seq.append(df[input_cols].iloc[i:i+window].values)
        y_seq.append(df[target_cols].iloc[i+window].values)
        D_seq.append(df['Dates'].iloc[i+window])
    return np.array(X_seq), np.array(y_seq), np.array(D_seq)

def train_model(model, train_loader, val_loader, optimizer, loss_fn, device, max_epochs=50, patience=5):
    train_losses, val_losses = [], []
    best_model, best_val_loss = None, float('inf')
    wait = 0

    for epoch in range(max_epochs):
        model.train()
        train_loss = 0
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            preds = model(xb)
            loss = loss_fn(preds, yb)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_losses.append(train_loss)

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                preds = model(xb)
                val_loss += loss_fn(preds, yb).item()
        val_losses.append(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model = model.state_dict()
            wait = 0
        else:
            wait += 1
            if wait >= patience:
                break

    model.load_state_dict(best_model)

    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Val Loss")
    plt.title("Loss Curve")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.show()

    return model

def plot_predictions(dates, y_true, y_pred, tenors):
    for i, tenor in enumerate(tenors):
        plt.figure(figsize=(10, 4))
        plt.plot(dates, y_true[:, i], label='Actual')
        plt.plot(dates, y_pred[:, i], '--', label='Predicted')
        plt.title(f"Yield: {tenor}")
        plt.xlabel("Date")
        plt.ylabel("Yield")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

def plot_yield_curves(y_true, y_pred, tenors):
    for i in range(0, len(y_pred), max(1, len(y_pred)//5)):
        plt.plot(tenors, y_true[i], label=f"True {i}")
        plt.plot(tenors, y_pred[i], '--', label=f"Pred {i}")
    plt.title("Yield Curve Shapes")
    plt.xlabel("Tenor")
    plt.ylabel("Yield")
    plt.legend()
    plt.grid(True)
    plt.show()

def predict_with_uncertainty(model, input_tensor, scaler_y, target_cols, n_samples=30):
    model.eval()
    preds = []
    for _ in range(n_samples):
        with torch.no_grad():
            pred = model(input_tensor).cpu().numpy()
            preds.append(pred)

    preds = np.array(preds).squeeze()
    mean_pred = preds.mean(axis=0)
    std_pred = preds.std(axis=0)

    lower = scaler_y.inverse_transform([mean_pred - 1.96 * std_pred])[0]
    upper = scaler_y.inverse_transform([mean_pred + 1.96 * std_pred])[0]
    mean = scaler_y.inverse_transform([mean_pred])[0]

    plt.plot(target_cols, mean, label='Prediction', marker='o')
    plt.fill_between(target_cols, lower, upper, alpha=0.3, label='95% CI')
    plt.title("Predicted Yield Curve with Uncertainty")
    plt.ylabel("Yield")
    plt.xlabel("Tenor")
    plt.grid(True)
    plt.legend()
    plt.show()

    return mean, lower, upper

# Pipeline execution
df = pd.read_csv("OIS_SPOT_ZAR.csv")
df.columns = df.columns.str.strip()
df['Dates'] = pd.to_datetime(df['Dates'])
df = df.sort_values("Dates")

# Drop specific bonds by name patterns
to_drop = [
    'SAGB 10.5 12/21/2026 Govt', 'SAGB 8 01/31/2030 Govt', 'SAGB 7 02/28/2031 Govt',
    'SAGB 8.25 03/31/2032 Govt', 'SAGB 10 03/31/2033 Govt', 'SAGB 8.875 02/28/2035 Govt',
    'SAGB 6.25 03/31/2036 Govt', 'SAGB 8.5 01/31/2037 Govt', 'SAGB 10.875 03/31/2038 Govt',
    'SAGB 9 01/31/2040 Govt', 'SAGB 6.5 02/28/2041 Govt', 'SAGB 8.75 01/31/2044 Govt',
    'SAGB 8.75 02/28/2048 Govt', 'SAGB 11.625 03/31/2053 Govt'
]
df.drop(columns=[col for col in to_drop if col in df.columns], inplace=True)

target_cols = ["1M", "2M", "3M", "4M", "5M", "6M", "7M", "8M", "9M", "1Y", "2Y", "3Y"]
input_cols = [col for col in df.columns if col not in target_cols + ['Dates']]

# Feature engineering: add yield spreads
if all(col in df.columns for col in ["3Y", "1M", "1Y", "6M", "2Y", "3M"]):
    df["3Y_1M_spread"] = df["3Y"] - df["1M"]
    df["1Y_6M_spread"] = df["1Y"] - df["6M"]
    df["2Y_3M_spread"] = df["2Y"] - df["3M"]
    input_cols += ["3Y_1M_spread", "1Y_6M_spread", "2Y_3M_spread"]

scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(df[input_cols].values)

pca = PCA(n_components=0.99)
X_pca = pca.fit_transform(X_scaled)
pca_cols = [f"PC{i+1}" for i in range(X_pca.shape[1])]

scaler_y = MinMaxScaler()
y_scaled = scaler_y.fit_transform(df[target_cols].values)

df_final = pd.concat([
    df[['Dates']].reset_index(drop=True),
    pd.DataFrame(X_pca, columns=pca_cols),
    pd.DataFrame(y_scaled, columns=target_cols)
], axis=1)

X_seq, y_seq, D_seq = create_sequences(df_final, pca_cols, target_cols, window=7)

split = int(0.8 * len(X_seq))
X_train, X_val = X_seq[:split], X_seq[split:]
y_train, y_val = y_seq[:split], y_seq[split:]
dates_val = D_seq[split:]

train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                                        torch.tensor(y_train, dtype=torch.float32)), batch_size=32, shuffle=True)

val_loader = DataLoader(TensorDataset(torch.tensor(X_val, dtype=torch.float32),
                                      torch.tensor(y_val, dtype=torch.float32)), batch_size=32)

model = YieldLSTM(input_dim=X_seq.shape[2], hidden_dim=64, output_dim=len(target_cols), dropout=0.2).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()

model = train_model(model, train_loader, val_loader, optimizer, loss_fn, device)

model.eval()
with torch.no_grad():
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)
    y_val_pred_scaled = model(X_val_tensor).cpu().numpy()

y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled)
y_val_true = scaler_y.inverse_transform(y_val)

r2_val = r2_score(y_val_true, y_val_pred)
mse_val = mean_squared_error(y_val_true, y_val_pred)

print(f"Validation R²: {r2_val:.4f}")
print(f"Validation MSE: {mse_val:.6f}")

with torch.no_grad():
    X_all_tensor = torch.tensor(X_seq, dtype=torch.float32).to(device)
    y_all_pred_scaled = model(X_all_tensor).cpu().numpy()
y_all_pred = scaler_y.inverse_transform(y_all_pred_scaled)
y_all_true = scaler_y.inverse_transform(y_seq)

r2_full = r2_score(y_all_true, y_all_pred)
mse_full = mean_squared_error(y_all_true, y_all_pred)
print(f"Full R²: {r2_full:.4f}")
print(f"Full MSE: {mse_full:.6f}")

plot_predictions(dates_val, y_val_true, y_val_pred, target_cols)
plot_yield_curves(y_val_true, y_val_pred, target_cols)

sample_input = torch.tensor(X_seq[-1:], dtype=torch.float32).to(device)
predict_with_uncertainty(model, sample_input, scaler_y, target_cols)

# SHAP analysis using surrogate model
from sklearn.ensemble import RandomForestRegressor
X_flat = X_seq.reshape((X_seq.shape[0], -1))
rf_model = RandomForestRegressor(n_estimators=50, random_state=42)
rf_model.fit(X_flat, y_seq[:, 0])  # just one tenor for SHAP demo
explainer = shap.Explainer(rf_model, X_flat)
shap_values = explainer(X_flat[:100])
shap.plots.beeswarm(shap_values, max_display=15)
