import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Set random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Step 1: Load and Preprocess Data
def load_and_preprocess_data(file_path="OIS_SPOT_ZAR.csv"):
    try:
        df = pd.read_csv(file_path)
        df.columns = df.columns.str.strip()
        df['Dates'] = pd.to_datetime(df['Dates'])
        df = df.sort_values('Dates')
        df.dropna(inplace=True)
        
        # Define input and output columns
        output_columns = ["1M", "2M", "3M", "4M", "5M", "6M", "7M", "8M", "9M", "1Y", "2Y", "3Y"]
        input_columns = [col for col in df.columns if col not in output_columns + ["Dates"]]
        
        # Normalize input features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(df[input_columns])
        
        # Apply PCA
        pca = PCA()
        X_pca = pca.fit_transform(X_scaled)
        
        # Select components explaining 97% variance
        explained_var = np.cumsum(pca.explained_variance_ratio_) * 100
        num_components = np.argmax(explained_var >= 97) + 1
        X_pca_reduced = X_pca[:, :num_components]
        
        return df, X_pca_reduced, output_columns, df['Dates']
    except FileNotFoundError:
        print("Data not loaded. Please provide the file 'OIS_SPOT_ZAR.csv'.")
        return None, None, None, None

# Step 2: Define Enhanced MLP Model
class EnhancedYieldMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout, num_layers=3):
        super(EnhancedYieldMLP, self).__init__()
        layers = []
        current_dim = input_dim
        for _ in range(num_layers):
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            current_dim = hidden_dim
        layers.append(nn.Linear(hidden_dim, output_dim))
        self.net = nn.Sequential(*layers)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.zeros_(module.bias)

    def forward(self, x, training=False):
        if training:
            self.train()
        else:
            self.eval()
        return self.net(x)

# Step 3: Training Function
def train_model(model, train_loader, optimizer, loss_fn, device, max_epochs=100, patience=10):
    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0

    for epoch in range(max_epochs):
        model.train()
        train_loss = 0
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            pred = model(xb, training=True)
            loss = loss_fn(pred, yb)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        if train_loss < best_val_loss:
            best_val_loss = train_loss
            best_model_state = model.state_dict()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                break
    
    model.load_state_dict(best_model_state)
    return model

# Step 4: Visualization Function for Individual Tenors
def plot_tenor(y_true, y_pred, dates, tenor_name, filename):
    dates = dates[-len(y_pred):]  # Match dates to y_pred length
    plt.figure(figsize=(12, 6))
    plt.plot(dates, y_true[-len(y_pred):], label="Actual")
    plt.plot(dates, y_pred, '--', label="Predicted")
    plt.title(f"Actual vs Predicted Yield - {tenor_name}")
    plt.xlabel("Date")
    plt.ylabel("Yield")
    plt.legend()
    plt.savefig(filename)
    plt.close()

# Step 5: Main Execution
def main():
    global y_scaler, output_columns
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Load and preprocess data
    df, X_pca_reduced, output_columns, dates = load_and_preprocess_data()
    if df is None:
        return
    
    # Scale y
    y_scaler = MinMaxScaler()
    y_scaled = y_scaler.fit_transform(df[output_columns].values)
    
    # Split data for evaluation (80-20), ensure test set has at least 2 samples
    total_samples = len(X_pca_reduced)
    train_size = max(int(0.8 * total_samples), total_samples - 2)
    X_train, X_test = X_pca_reduced[:train_size], X_pca_reduced[train_size:]
    y_train, y_test = y_scaled[:train_size], y_scaled[train_size:]
    dates_train, dates_test = dates[:train_size], dates[train_size:]
    
    print("X_pca_reduced shape:", X_pca_reduced.shape)
    print("y_scaled shape:", y_scaled.shape)
    print("dates shape:", dates.shape)
    print("Train size:", train_size)
    print("Test size:", len(X_pca_reduced) - train_size)
    
    # Prepare full dataset for training
    train_dataset = TensorDataset(
        torch.tensor(X_pca_reduced, dtype=torch.float32),
        torch.tensor(y_scaled, dtype=torch.float32)
    )
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    
    # Initialize model with best parameters
    model = EnhancedYieldMLP(
        input_dim=X_pca_reduced.shape[1],
        hidden_dim=138,
        output_dim=len(output_columns),
        dropout=0.1003,
        num_layers=5
    ).to(device)
    
    # Train model on full dataset
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.003671, weight_decay=1e-5)
    loss_fn = nn.MSELoss()
    model = train_model(model, train_loader, optimizer, loss_fn, device)
    
    # Predict on full and test data
    model.eval()
    with torch.no_grad():
        all_preds_scaled = model(torch.tensor(X_pca_reduced, dtype=torch.float32).to(device)).cpu().numpy()
        test_preds_scaled = model(torch.tensor(X_test, dtype=torch.float32).to(device)).cpu().numpy()
    all_preds = y_scaler.inverse_transform(all_preds_scaled)
    test_preds = y_scaler.inverse_transform(test_preds_scaled)
    
    # Visualizations for each tenor
    for i, tenor in enumerate(output_columns):
        plot_tenor(df[tenor].values, all_preds[:, i], dates, tenor, f"{tenor}_yield.png")
        plot_tenor(df[tenor].values[train_size:], test_preds[:, i], dates[train_size:], tenor, f"{tenor}_test_yield.png")
    
    # Calculate and print R² and MSE scores
    if len(y_test) < 2:
        print("Warning: Test set has less than 2 samples, R² and MSE for test not calculated.")
        r2_test = float('nan')
        mse_test = float('nan')
    else:
        r2_test = r2_score(y_test, test_preds)
        mse_test = mean_squared_error(y_test, test_preds)
    
    r2_full = r2_score(y_scaled, all_preds_scaled)
    mse_full = mean_squared_error(y_scaled, all_preds_scaled)
    
    print(f"R² Test: {r2_test:.4f}")
    print(f"R² Full: {r2_full:.4f}")
    print(f"MSE Test: {mse_test:.4f}")
    print(f"MSE Full: {mse_full:.4f}")

if __name__ == "__main__":
    main()
