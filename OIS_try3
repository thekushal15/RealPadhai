
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Reload + minimal memory preprocessing
df = pd.read_csv("OIS_SPOT_ZAR.csv")
df.columns = df.columns.str.strip()
df['Dates'] = pd.to_datetime(df['Dates'])
df = df.sort_values("Dates").dropna()

# Define columns
target_cols = ["1M", "2M", "3M", "4M", "5M", "6M", "7M", "8M", "9M", "1Y", "2Y", "3Y"]
input_cols = [col for col in df.columns if col not in target_cols + ['Dates']]

# ‚è¨ Downcast types to save RAM
df[input_cols] = df[input_cols].apply(pd.to_numeric, downcast="float")
df[target_cols] = df[target_cols].apply(pd.to_numeric, downcast="float")

# ‚úÖ Step 1: Scale inputs (StandardScaler)
X_raw = df[input_cols].values.astype(np.float32)
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X_raw)

# ‚úÖ Step 2: Apply PCA
pca = PCA(n_components=0.97)  # keep 97% variance
X_pca = pca.fit_transform(X_scaled)
print(f"PCA reduced input dim to: {X_pca.shape[1]}")

# ‚úÖ Step 3: Scale targets (MinMaxScaler)
scaler_y = MinMaxScaler()
y_scaled = scaler_y.fit_transform(df[target_cols].values.astype(np.float32))

# ‚úÖ Step 4: Rebuild DataFrame for sequencing
pca_cols = [f"PC{i+1}" for i in range(X_pca.shape[1])]
df_pca = pd.DataFrame(X_pca, columns=pca_cols)
df_final = pd.concat([df[['Dates']].reset_index(drop=True), df_pca, pd.DataFrame(y_scaled, columns=target_cols)], axis=1)


# Use updated pca_cols as input columns
X_seq, y_seq, D_seq = create_sequences(df_final, input_cols=pca_cols, target_cols=target_cols, window=7)

# Sanity check
print("X_seq:", X_seq.shape)  # (N, 7, features)
print("y_seq:", y_seq.shape)  # (N, 12)














import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import r2_score, mean_squared_error

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# Reproducibility
torch.manual_seed(42)
np.random.seed(42)

# ‚öôÔ∏è Step 1: Sequence Builder
def create_sequences(df, input_cols, target_cols, window=7):
    X_seq, Y_seq, D_seq = [], [], []
    for i in range(len(df) - window):
        X_seq.append(df[input_cols].iloc[i:i+window].values)
        Y_seq.append(df[target_cols].iloc[i+window].values)
        D_seq.append(df['Dates'].iloc[i+window])
    return np.array(X_seq), np.array(Y_seq), np.array(D_seq)

# üß† Step 2: LSTM Model
class YieldLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=2, dropout=dropout, bidirectional=True)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim*2, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, output_dim)
        )
    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]  # last timestep
        return self.fc(out)

# üèãÔ∏è‚Äç‚ôÇÔ∏è Step 3: Training
def train_model(model, train_loader, val_loader, optimizer, loss_fn, device, max_epochs=50, patience=5):
    train_losses, val_losses = [], []
    best_model, best_val_loss = None, float('inf')
    wait = 0

    for epoch in range(max_epochs):
        model.train()
        running_loss = 0
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            preds = model(xb)
            loss = loss_fn(preds, yb)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        train_losses.append(running_loss)

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                preds = model(xb)
                val_loss += loss_fn(preds, yb).item()
        val_losses.append(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model = model.state_dict()
            wait = 0
        else:
            wait += 1
            if wait >= patience:
                print(f"Early stopped at epoch {epoch}")
                break

    model.load_state_dict(best_model)

    # üîç Plot training curve
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Val Loss")
    plt.title("Loss Curve")
    plt.xlabel("Epoch")
    plt.ylabel("MSE Loss")
    plt.legend()
    plt.grid(True)
    plt.show()

    return model

# üìä Step 4: Plot Tenor-wise Prediction
def plot_predictions(dates, y_true, y_pred, tenors):
    for i, tenor in enumerate(tenors):
        plt.figure(figsize=(10, 4))
        plt.plot(dates, y_true[:, i], label='Actual')
        plt.plot(dates, y_pred[:, i], '--', label='Predicted')
        plt.title(f"Yield: {tenor}")
        plt.xlabel("Date")
        plt.ylabel("Yield")
        plt.legend()
        plt.tight_layout()
        plt.grid(True)
        plt.show()

# üìà Step 5: Plot Yield Curve Shapes
def plot_yield_curves(y_true, y_pred, tenors):
    for i in range(0, len(y_pred), max(1, len(y_pred)//5)):  # plot 5 samples
        plt.plot(tenors, y_true[i], label=f"True {i}")
        plt.plot(tenors, y_pred[i], linestyle='--', label=f"Pred {i}")
    plt.title("Yield Curve Shapes (sample dates)")
    plt.xlabel("Tenor")
    plt.ylabel("Yield")
    plt.legend()
    plt.grid(True)
    plt.show()










def run_yield_lstm_pipeline(file_path="OIS_SPOT_ZAR.csv"):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    df = pd.read_csv(file_path)
    df.columns = df.columns.str.strip()
    df['Dates'] = pd.to_datetime(df['Dates'])
    df.sort_values('Dates', inplace=True)
    df.dropna(inplace=True)

    # üîª Define columns
    target_cols = ["1M", "2M", "3M", "4M", "5M", "6M", "7M", "8M", "9M", "1Y", "2Y", "3Y"]
    input_cols = [col for col in df.columns if col not in target_cols + ['Dates']]

    # üßº Scale features
    X_scaler = StandardScaler()
    df[input_cols] = X_scaler.fit_transform(df[input_cols])

    y_scaler = MinMaxScaler()
    df[target_cols] = y_scaler.fit_transform(df[target_cols])

    # üîÅ Create sequences
    X, y, dates = create_sequences(df, input_cols, target_cols, window=7)

    # ü™ú Train-test split (rolling)
    split_idx = int(0.8 * len(X))
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    dates_val = dates[split_idx:]

    # üéØ Dataloaders
    train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                                            torch.tensor(y_train, dtype=torch.float32)), batch_size=32, shuffle=True)
    val_loader = DataLoader(TensorDataset(torch.tensor(X_val, dtype=torch.float32),
                                          torch.tensor(y_val, dtype=torch.float32)), batch_size=32)

    # ‚öôÔ∏è Model
    model = YieldLSTM(input_dim=X.shape[2], hidden_dim=64, output_dim=len(target_cols), dropout=0.2).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    loss_fn = nn.MSELoss()

    # üèãÔ∏è Train
    model = train_model(model, train_loader, val_loader, optimizer, loss_fn, device)

    # üìà Predict
    model.eval()
    with torch.no_grad():
        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)
        y_pred = model(X_val_tensor).cpu().numpy()
        y_val_true = y_val
        y_pred_true = y_scaler.inverse_transform(y_pred)
        y_val_true = y_scaler.inverse_transform(y_val_true)

    # üìä Visualize
    plot_predictions(dates_val, y_val_true, y_pred_true, target_cols)
    plot_yield_curves(y_val_true, y_pred_true, target_cols)

    # üßÆ Metrics
    r2 = r2_score(y_val_true, y_pred_true)
    mse = mean_squared_error(y_val_true, y_pred_true)
    print(f"R¬≤ Score (Val): {r2:.4f}")
    print(f"MSE (Val): {mse:.6f}")











run_yield_lstm_pipeline("OIS_SPOT_ZAR.csv")
