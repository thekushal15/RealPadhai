import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from copy import deepcopy

# Set random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Step 1: Load and Preprocess Data
def load_and_preprocess_data(file_path="OIS_SPOT_ZAR.csv"):
    try:
        df = pd.read_csv(file_path)
        df.columns = df.columns.str.strip()
        df['Dates'] = pd.to_datetime(df['Dates'])
        df = df.sort_values('Dates')
        df.dropna(inplace=True)
        
        # Define input and output columns
        output_columns = ["1M", "2M", "3M", "4M", "5M", "6M", "7M", "8M", "9M", "1Y", "2Y", "3Y"]
        input_columns = [col for col in df.columns if col not in output_columns + ["Dates"]]
        
        # Normalize input features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(df[input_columns])
        
        # Apply PCA
        pca = PCA()
        X_pca = pca.fit_transform(X_scaled)
        
        # Select components explaining 97% variance
        explained_var = np.cumsum(pca.explained_variance_ratio_) * 100
        num_components = np.argmax(explained_var >= 97) + 1
        X_pca_reduced = X_pca[:, :num_components]
        
        return df, X_pca_reduced, output_columns, df['Dates']
    except FileNotFoundError:
        print("Data not loaded. Please provide the file 'OIS_SPOT_ZAR.csv'.")
        return None, None, None, None

# Step 2: Outlier Filtering with Percentage Change
def filter_outliers(df, output_columns):
    """Calculate percentage change for each output and remove top 3% extreme movements."""
    # Calculate percentage change for each output column
    percentage_changes = df[output_columns].pct_change().abs() * 100  # Convert to percentage
    # Create a new column with maximum percentage change per row
    df['max_pct_change'] = percentage_changes.max(axis=1)
    # Calculate 97th percentile threshold
    threshold = df['max_pct_change'].quantile(0.97)
    # Remove rows where max percentage change exceeds threshold
    df_filtered = df[df['max_pct_change'] <= threshold].copy()
    # Drop the temporary column
    df_filtered = df_filtered.drop(columns=['max_pct_change'])
    return df_filtered

# Step 3: Prepare Time Series Data
def prepare_time_series_data(X_pca, y, seq_length=7):
    """Create sequences for time series model."""
    X_seq, y_seq = [], []
    for i in range(len(X_pca) - seq_length):
        X_seq.append(X_pca[i:i+seq_length].flatten())
        y_seq.append(y[i+seq_length])
    return np.array(X_seq), np.array(y_seq)

# Step 4: Define Enhanced MLP Model
class EnhancedYieldMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout, num_layers=3):
        super(EnhancedYieldMLP, self).__init__()
        layers = []
        current_dim = input_dim
        for _ in range(num_layers):
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            current_dim = hidden_dim
        layers.append(nn.Linear(hidden_dim, output_dim))
        self.net = nn.Sequential(*layers)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.zeros_(module.bias)

    def forward(self, x, training=False):
        if training:
            self.train()
        else:
            self.eval()
        return self.net(x)

# Step 5: Training Function
def train_model(model, train_loader, optimizer, loss_fn, device, max_epochs=100, patience=10):
    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0

    for epoch in range(max_epochs):
        model.train()
        train_loss = 0
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            pred = model(xb, training=True)
            loss = loss_fn(pred, yb)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        if train_loss < best_val_loss:
            best_val_loss = train_loss
            best_model_state = deepcopy(model.state_dict())
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                break
    
    model.load_state_dict(best_model_state)
    return model

# Step 6: Monte Carlo Dropout for Uncertainty
def mc_dropout_predictions(model, X_tensor, n_samples=100):
    model.train()  # Enable dropout during inference
    predictions = []
    for _ in range(n_samples):
        with torch.no_grad():
            pred = model(X_tensor, training=True).cpu().numpy()
            predictions.append(pred)
    predictions = np.stack(predictions)
    mean_preds = np.mean(predictions, axis=0)
    std_preds = np.std(predictions, axis=0)
    return mean_preds, std_preds

# Step 7: Visualization Functions
def plot_actual_vs_predicted(y_true, y_pred, dates, title, filename):
    dates = dates[-len(y_pred):]  # Match dates to y_pred length
    plt.figure(figsize=(12, 6))
    for i in range(y_true.shape[1]):
        plt.plot(dates, y_true[-len(y_pred):, i], label=f"Actual Tenor {output_columns[i]}")
        plt.plot(dates, y_pred[:, i], '--', label=f"Predicted Tenor {output_columns[i]}")
    plt.title(title)
    plt.xlabel("Date")
    plt.ylabel("Yield")
    plt.legend()
    plt.savefig(filename)
    plt.close()

def plot_confidence_intervals(y_true, mean_preds, std_preds, dates, title, filename):
    dates = dates[-len(mean_preds):]  # Match dates to mean_preds length
    plt.figure(figsize=(12, 6))
    for i in range(y_true.shape[1]):
        plt.plot(dates, y_true[-len(mean_preds):, i], label=f"Actual Tenor {output_columns[i]}")
        plt.plot(dates, mean_preds[:, i], '--', label=f"Predicted Tenor {output_columns[i]}")
        plt.fill_between(
            dates,
            mean_preds[:, i] - 1.96 * std_preds[:, i],
            mean_preds[:, i] + 1.96 * std_preds[:, i],
            alpha=0.2
        )
    plt.title(title)
    plt.xlabel("Date")
    plt.ylabel("Yield")
    plt.legend()
    plt.savefig(filename)
    plt.close()

def plot_tenor_spreads(y_true, y_pred, tenors, filename):
    spreads = y_true[-len(y_pred):] - y_pred  # Match lengths
    plt.figure(figsize=(12, 6))
    sns.boxplot(data=spreads)
    plt.xticks(np.arange(len(tenors)), tenors)
    plt.title("Tenor-wise Prediction Spreads")
    plt.ylabel("Prediction Error")
    plt.savefig(filename)
    plt.close()

def plot_r2_over_time(y_true, y_pred, dates, filename):
    dates = dates[-len(y_pred):]  # Match dates to y_pred length
    r2_scores = [r2_score(y_true[i:i+1, :], y_pred[i:i+1, :]) for i in range(len(y_pred)) if i+1 < len(y_true)]
    plt.figure(figsize=(12, 6))
    plt.plot(dates, r2_scores)
    plt.title("R² Score Over Time")
    plt.xlabel("Date")
    plt.ylabel("R² Score")
    plt.savefig(filename)
    plt.close()

# Step 8: Main Execution
def main():
    global y_scaler, output_columns
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Load and preprocess data
    df, X_pca_reduced, output_columns, dates = load_and_preprocess_data()
    if df is None:
        return
    
    # Filter outliers based on percentage change
    df_filtered = filter_outliers(df, output_columns)
    X_pca_filtered = X_pca_reduced[df_filtered.index]
    y = df_filtered[output_columns].values
    dates_filtered = dates[df_filtered.index]
    
    print("X_pca_filtered shape:", X_pca_filtered.shape)
    print("y shape:", y.shape)
    print("dates_filtered shape:", dates_filtered.shape)
    
    # Scale y
    y_scaler = MinMaxScaler()
    y_scaled = y_scaler.fit_transform(y)
    
    # Split data for evaluation (80-20)
    train_size = int(0.8 * len(X_pca_filtered))
    X_train, X_test = X_pca_filtered[:train_size], X_pca_filtered[train_size:]
    y_train, y_test = y_scaled[:train_size], y_scaled[train_size:]
    dates_train, dates_test = dates_filtered[:train_size], dates_filtered[train_size:]
    
    print("Train size:", train_size)
    print("Test size:", len(X_pca_filtered) - train_size)
    
    # Prepare full dataset for training
    train_dataset = TensorDataset(
        torch.tensor(X_pca_filtered, dtype=torch.float32),
        torch.tensor(y_scaled, dtype=torch.float32)
    )
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    
    # Initialize model with best parameters
    model = EnhancedYieldMLP(
        input_dim=X_pca_reduced.shape[1],
        hidden_dim=138,
        output_dim=y.shape[1],
        dropout=0.1003,
        num_layers=5
    ).to(device)
    
    # Train model on full dataset
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.003671, weight_decay=1e-5)
    loss_fn = nn.MSELoss()
    model = train_model(model, train_loader, optimizer, loss_fn, device)
    
    # Predict on full and test data
    model.eval()
    with torch.no_grad():
        all_preds_scaled = model(torch.tensor(X_pca_filtered, dtype=torch.float32).to(device)).cpu().numpy()
        test_preds_scaled = model(torch.tensor(X_test, dtype=torch.float32).to(device)).cpu().numpy()
    all_preds = y_scaler.inverse_transform(all_preds_scaled)
    test_preds = y_scaler.inverse_transform(test_preds_scaled)
    
    # Monte Carlo Dropout for Uncertainty
    mean_preds, std_preds = mc_dropout_predictions(model, torch.tensor(X_pca_filtered, dtype=torch.float32).to(device))
    mean_preds = y_scaler.inverse_transform(mean_preds)
    
    # Visualizations
    plot_actual_vs_predicted(y, all_preds, dates_filtered, "Actual vs Predicted Yield Curves", "yield_curves.png")
    plot_confidence_intervals(y, mean_preds, std_preds, dates_filtered, "Yield Curves with Confidence Intervals", "confidence_intervals.png")
    plot_tenor_spreads(y, all_preds, output_columns, "tenor_spreads.png")
    plot_r2_over_time(y, all_preds, dates_filtered, "r2_over_time.png")
    
    # Calculate and print R² and MSE scores
    if len(y[train_size:]) < 2:
        print("Warning: Test set has less than 2 samples, R² and MSE for test not calculated.")
        r2_test = float('nan')
        mse_test = float('nan')
    else:
        r2_test = r2_score(y[train_size:], test_preds)
        mse_test = mean_squared_error(y[train_size:], test_preds)
    
    r2_full = r2_score(y, all_preds)
    mse_full = mean_squared_error(y, all_preds)
    
    print(f"R² Test: {r2_test:.4f}")
    print(f"R² Full: {r2_full:.4f}")
    print(f"MSE Test: {mse_test:.4f}")
    print(f"MSE Full: {mse_full:.4f}")

if __name__ == "__main__":
    main()
